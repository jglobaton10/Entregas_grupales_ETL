{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c5b411f-b096-4a7c-a2b7-828ac869443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import functions as f\n",
    "from pymssql import _mssql\n",
    "from pyspark.sql.functions import monotonically_increasing_id as mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a9c874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "appName = \"PySpark SQL Server via JDBC\"\n",
    "master = \"local\"\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(appName) \\\n",
    "    .setMaster(master) \\\n",
    "    .set(\"spark.driver.extraClassPath\",\"C:\\\\Users\\\\estudiante\\\\mssql-jdbc-9.2.1.jre8.jar\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sql_context = SQLContext(sc)\n",
    "spark = sql_context.sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca2908c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "server='localhost:1433'\n",
    "database = \"ProyectoAeropuertosSemana8\"\n",
    "user = \"sa\"\n",
    "password  = \"12345\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29268014",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = _mssql.connect(server=server, user=user, password=password,database=database)\n",
    "query = f\"-- Create schemas\\\n",
    "-- Create tables\\\n",
    "IF (NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = 'DimFecha'))\\\n",
    "BEGIN\\\n",
    "  CREATE TABLE DimFecha\\\n",
    "  (\\\n",
    "    IDFecha INT NOT NULL,\\\n",
    "    Año VARCHAR(4),\\\n",
    "    Mes VARCHAR(2),\\\n",
    "    PRIMARY KEY(IDFecha)\\\n",
    "  )\\\n",
    "END;\\\n",
    "\\\n",
    "IF (NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = 'DimTipo_Equipo'))\\\n",
    "BEGIN\\\n",
    "  CREATE TABLE DimTipo_Equipo\\\n",
    "  (\\\n",
    "    IDEquipo INT NOT NULL,\\\n",
    "    NombreEquipo VARCHAR(4),\\\n",
    "    PRIMARY KEY(IDEquipo)\\\n",
    "  )\\\n",
    "END;\\\n",
    "\\\n",
    "IF (NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = 'DimTipoVuelo'))\\\n",
    "BEGIN\\\n",
    "  CREATE TABLE DimTipoVuelo\\\n",
    "  (\\\n",
    "    IDTipoVuelo INT NOT NULL,\\\n",
    "    CodigoVuelo VARCHAR(1),\\\n",
    "    TipoVuelo VARCHAR(10),\\\n",
    "    PRIMARY KEY(IDTipoVuelo)\\\n",
    "  )\\\n",
    "END;\\\n",
    "\\\n",
    "IF (NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = 'DimTipo_Trafico'))\\\n",
    "BEGIN\\\n",
    "  CREATE TABLE DimTipo_Trafico\\\n",
    "  (\\\n",
    "    IDTipoTrafico INT NOT NULL,\\\n",
    "    Codigo_Trafico VARCHAR(1),\\\n",
    "    Descripcion VARCHAR(10),\\\n",
    "    PRIMARY KEY(IDTipoTrafico)\\\n",
    "  )\\\n",
    "END;\\\n",
    "\\\n",
    "IF (NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = 'DimEmpresaTransportadora'))\\\n",
    "BEGIN\\\n",
    "  CREATE TABLE DimEmpresaTransportadora\\\n",
    "  (\\\n",
    "    IDEmpresa INT NOT NULL,\\\n",
    "    NombreEmpresa VARCHAR(50),\\\n",
    "    PRIMARY KEY(IDEmpresa)\\\n",
    "  )\\\n",
    "END;\\\n",
    "\\\n",
    "IF (NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = 'FactVuelos'))\\\n",
    "BEGIN\\\n",
    "  CREATE TABLE FactVuelos\\\n",
    "  (\\\n",
    "    ID INT NOT NULL,\\\n",
    "    IDFecha INT,\\\n",
    "    IDTipoEquipo INT,\\\n",
    "    IDAeropuertoOrigen INT,\\\n",
    "    IDAeropuertoDestino INT,\\\n",
    "    IDTipoVuelo INT,\\\n",
    "    IDTipoTrafico INT,\\\n",
    "    IDEmpresa INT,\\\n",
    "    Vuelos INT,\\\n",
    "    Pasajeros INT,\\\n",
    "    CargaBordo INT,\\\n",
    "    TotalSillas INT,\\\n",
    "    TotalCarga INT,\\\n",
    "    PRIMARY KEY(ID)\\\n",
    "  )\\\n",
    "END;\\\n",
    "\\\n",
    "IF (NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = 'DimAeropuerto'))\\\n",
    "BEGIN\\\n",
    "  CREATE TABLE DimAeropuerto\\\n",
    "  (\\\n",
    "    IDAeropuerto INT NOT NULL,\\\n",
    "    Sigla CHARACTER(3),\\\n",
    "    IATA CHARACTER(3),\\\n",
    "    Ubicacion VARCHAR(50),\\\n",
    "    NombreAeropuerto VARCHAR(50),\\\n",
    "    Municipio VARCHAR(50),\\\n",
    "    Departamento VARCHAR(50),\\\n",
    "    Pais VARCHAR(50),\\\n",
    "    Categoria VARCHAR(50),\\\n",
    "    Latitud FLOAT,\\\n",
    "    Longitud FLOAT,\\\n",
    "    Propietario VARCHAR(50),\\\n",
    "    Explotador VARCHAR(50),\\\n",
    "    LongitudPista INT,\\\n",
    "    AnchoPista INT,\\\n",
    "    PBMO INT,\\\n",
    "    Elevacion INT,\\\n",
    "    Resolucion VARCHAR(50),\\\n",
    "    Clase VARCHAR(50),\\\n",
    "    Tipo VARCHAR(50),\\\n",
    "    GCD_Municipio VARCHAR(50),\\\n",
    "    GCD_Departamento VARCHAR(50),\\\n",
    "    FechaInicioVigencia DATE,\\\n",
    "    FechaFinVigencia DATE,\\\n",
    "    VersionDelRegistro VARCHAR(1),\\\n",
    "    Anio INT,\\\n",
    "    IDPIB INT,\\\n",
    "    PRIMARY KEY(IDAeropuerto)\\\n",
    "  )\\\n",
    "END;\\\n",
    "IF (NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = 'DimInformacionPIB'))\\\n",
    "BEGIN\\\n",
    "CREATE TABLE DimInformacionPIB\\\n",
    "(\\\n",
    "    IDPIB INT NOT NULL,\\\n",
    "    PIB INT,\\\n",
    "    FechaInicioVigencia DATE,\\\n",
    "    FechaFinVigencia DATE,\\\n",
    "    VersionRegistro VARCHAR(1),\\\n",
    "    PRIMARY KEY(IDPIB)\\\n",
    ")\\\n",
    "END;\"\n",
    "conn.execute_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc84d956",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfvuelos = spark.read.format(\"csv\").load(\"vuelosEtapa3.csv\",format=\"csv\",sep=\",\",\n",
    "                                         inferSchema='true',header='true')\n",
    "dfaeropuertos =spark.read.format(\"csv\").load(\"aeropuertosEtapa3.csv\",format=\"csv\",sep=\";\",\n",
    "                                         inferSchema='true',header='true')\n",
    "dfaeropuertosdelmundo=spark.read.format(\"csv\").load(\"Aeropuertosdelmundo.csv\",format=\"csv\",sep=\";\",\n",
    "                                         inferSchema='true',header='true')\n",
    "dfPIB=spark.read.format(\"csv\").load(\"InformacionPIB.csv\",format=\"csv\",sep=\",\",\n",
    "                                         inferSchema='true',header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2309df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sequence, to_date, explode, col,when,lit,expr,substring,regexp_replace\n",
    "from pyspark.sql import functions as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d76c4b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agregar nombres de los aeropuertos internacionales\n",
    "dfaeropuertosdelmundo=dfaeropuertosdelmundo.dropDuplicates()\n",
    "dfaeropuertosdelmundo_origen=dfaeropuertosdelmundo.selectExpr('Origen as sigla', 'Ciudad_Origen as municipio',\n",
    "                                                              'APTO_ORIGEN as nombre', 'Pais_Origen as pais')\n",
    "dfaeropuertosdelmundo_destino=dfaeropuertosdelmundo.selectExpr('Destino as sigla', 'Ciudad_Destino as municipio',\n",
    "                                                              'APTO_DESTINO as nombre', 'Pais_Destino as pais')\n",
    "dfaeropuertosdelmundo = dfaeropuertosdelmundo_origen.union(dfaeropuertosdelmundo_destino).where(\"pais is not null\") \n",
    "dfaeropuertosdelmundo=dfaeropuertosdelmundo.dropDuplicates().filter(dfaeropuertosdelmundo.pais!=\"COLOMBIA\")\n",
    "dfaeropuertosdelmundo1=dfaeropuertosdelmundo.filter(dfaeropuertosdelmundo.pais!=\"COLOMBIA\")\n",
    "dfaeropuertosdelmundo2=dfaeropuertosdelmundo.filter(dfaeropuertosdelmundo.pais==\"COLOMBIA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6aa1b203",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfaeropuertos=dfaeropuertos.withColumn(\"pais\",lit(\"COLOMBIA\")).withColumn(\"ubicacion\",lit(\"Nacional\"))\n",
    "dfaeropuertosdelmundo1=dfaeropuertosdelmundo1.withColumn(\"Ano\",lit(\"2013\")).withColumn(\"ubicacion\",lit(\"Internacional\"))\n",
    "dfaeropuertosdelmundo2=dfaeropuertosdelmundo2.withColumn(\"Ano\",lit(\"2013\")).withColumn(\"ubicacion\",lit(\"Nacional\"))\n",
    "dfaeropuertos1=dfaeropuertosdelmundo1.unionByName(dfaeropuertosdelmundo2, allowMissingColumns=True)\n",
    "dfaeropuertos=dfaeropuertos.unionByName(dfaeropuertosdelmundo1, allowMissingColumns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61f8419a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TransformacionInformacionPIB\n",
    "dfPIBUnpivot=dfPIB.withColumnRenamed(\"DEPARTAMENTOS\",\"Departamento\").withColumnRenamed(\"2014\",\"Ano2014\").withColumnRenamed(\"2015\",\"Ano2015\").withColumnRenamed(\"2016\",\"Ano2016\")\\\n",
    "             .withColumnRenamed(\"2017\",\"Ano2017\").withColumnRenamed(\"2018\",\"Ano2018\").withColumnRenamed(\"2019\",\"Ano2019\")\\\n",
    "             .withColumnRenamed(\"2020\",\"Ano2020\")\n",
    "unpivotExpr = \"stack(7, 'Ano2014',Ano2014, 'Ano2015', Ano2015, 'Ano2016', Ano2016,'Ano2017', Ano2017,'Ano2018', Ano2018,'Ano2019', Ano2019,'Ano2020', Ano2020) as (Anio,PIB)\"\n",
    "dfPIBUnpivot = dfPIBUnpivot.selectExpr(\"Codigo\",\"Departamento\", unpivotExpr).where(\"PIB is not null\")    \n",
    "dfPIBUnpivot=dfPIBUnpivot.withColumn('Anio', substring('Anio', 4,4))\n",
    "dfPIBUnpivot=dfPIBUnpivot.withColumn('Departamento', \n",
    "    when(dfPIBUnpivot.Departamento.startswith('San Andrés'),'San Andrés islas') \\\n",
    "   .when(dfPIBUnpivot.Departamento.startswith('BOG'),'Bogotá, D.C.') \\\n",
    "   .otherwise(dfPIBUnpivot.Departamento))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fe47f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfBIPparte1 = dfPIBUnpivot.filter(dfPIBUnpivot.Anio==\"2014\")\n",
    "dfBIPparte2 = dfPIBUnpivot.filter(dfPIBUnpivot.Anio==\"2015\")\n",
    "dfBIPparte3 = dfPIBUnpivot.filter(dfPIBUnpivot.Anio==\"2016\")\n",
    "dfBIPparte4 = dfPIBUnpivot.filter(dfPIBUnpivot.Anio==\"2017\")\n",
    "dfBIPparte5 = dfPIBUnpivot.filter(dfPIBUnpivot.Anio==\"2018\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70bb9b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_table(spark, jdbc_hostname, database, data_table, username, password):\n",
    "    jdbc_url = \"jdbc:sqlserver://{0};database={1}\".format(jdbc_hostname, database)\n",
    "\n",
    "    connection_details = {\n",
    "        \"user\": username,\n",
    "        \"password\": password,\n",
    "        \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n",
    "    }\n",
    "\n",
    "    df = spark.read.jdbc(url=jdbc_url, table=data_table, properties=connection_details)\n",
    "    return df\n",
    "\n",
    "def write_table(spark, jdbc_hostname, database, data_table, username, password,mode,df):\n",
    "    jdbc_url = \"jdbc:sqlserver://{0};database={1}\".format(jdbc_hostname, database)\n",
    "\n",
    "    connection_details = {\n",
    "        \"user\": username,\n",
    "        \"password\": password,\n",
    "        \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n",
    "        \n",
    "        \n",
    "    }\n",
    "\n",
    "    df.write.jdbc(url=jdbc_url, table=data_table, properties=connection_details, mode=mode)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b3c77e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "def actualizar_historia_PIB(df_a_cargar):\n",
    "    tablaDWH = load_table(spark, server, database, 'DimInformacionPIB', user, password)\n",
    "    \n",
    "    df=df_a_cargar.selectExpr('Departamento as Departamento', 'Anio as Anio', 'PIB as PIB')\n",
    "    if tablaDWH.count()==0:\n",
    "        \n",
    "        df=df.withColumn(\"FechaInicioVigencia\",lit(\"1900-01-01\"))\n",
    "        df=df.withColumn(\"FechaFinVigencia\",lit(\"2300-01-01\"))\n",
    "        df=df.withColumn(\"VersionDelRegistro\",lit(\"S\"))\n",
    "        df=df.dropDuplicates()\n",
    "        df=df.sort(col(\"Departamento\"))\n",
    "        df = df.coalesce(1).withColumn(\"IDPIB\", mid())\n",
    "        df.createOrReplaceTempView(\"df\")\n",
    "        df = spark.sql(\"SELECT INT(IDPIB),STRING(Departamento),INT(PIB), DATE(FechaInicioVigencia),\\\n",
    "                       DATE(FechaFinVigencia), STRING(VersionDelRegistro),INT(Anio) from df\")\n",
    "        x=df.count()\n",
    "        df=write_table(spark, server, database, 'DimInformacionPIB', user,password,'overwrite',df)\n",
    "    else:\n",
    "        tablaDWH.persist()\n",
    "        df_fechas_antiguas=tablaDWH.selectExpr('Departamento','Departamento as DepartamentoAnt')\n",
    "        df_fechas_antiguas=df_fechas_antiguas.dropDuplicates()\n",
    "        df_nuevo=df.selectExpr('Departamento','Anio as AnioNuevo')\n",
    "        df_nuevo=df_nuevo.dropDuplicates()\n",
    "        df_temp=tablaDWH.join(df_nuevo, how = 'left', on = 'Departamento')\n",
    "        #registros que no tienen actualizacion\n",
    "        df_temp.persist()\n",
    "        df_mantener=df_temp.filter(df_temp.AnioNuevo.isNull())\n",
    "        df_mantener=df_mantener.drop('AnioNuevo')\n",
    "        df_mantener=df_mantener.withColumn(\"Origen\",lit(\"Mantener\"))\n",
    "        #registros que si tienen actualización\n",
    "        df_actualizar=df_temp.filter(df_temp.AnioNuevo.isNotNull())\n",
    "        #registros viejos que no van a cambiar\n",
    "        df_actualizar.persist()\n",
    "        df_actualizar_registrosviejos=df_actualizar.filter(df_actualizar.VersionDelRegistro==\"N\")\n",
    "        df_actualizar_registrosviejos=df_actualizar_registrosviejos.drop('AnioNuevo')\n",
    "        #actualización de registros que eran vigentes\n",
    "        df_actualizar_registrosvigentes=df_actualizar.filter(df_actualizar.VersionDelRegistro==\"S\")\n",
    "        df_actualizar_registrosvigentes=df_actualizar_registrosvigentes.withColumn(\"VersionDelRegistro\",lit(\"N\"))\n",
    "        df_actualizar_registrosvigentes=df_actualizar_registrosvigentes.withColumn(\"FechaFinVigencia\",\n",
    "                                                                                    sf.concat(sf.col('Anio'),sf.lit('-12-31'))\n",
    "                                                                                   )    \n",
    "        df_actualizar_registrosvigentes=df_actualizar_registrosvigentes.drop('AnioNuevo')\n",
    "        #registros nuevos para ingresar a la base\n",
    "        #Encontar llave máxima\n",
    "        max_key = tablaDWH.agg({\"IDPIB\": \"max\"}).collect()[0][0]\n",
    "        df_nuevos_registros=df.alias('df_nuevos_registros')\n",
    "        df_nuevos_registros=df_nuevos_registros.join(df_fechas_antiguas,how = 'left', on = 'Departamento')\n",
    "        df_nuevos_registros=df_nuevos_registros.withColumn(\"FechaInicioVigencia\",when(df_nuevos_registros.DepartamentoAnt.isNull(),\\\n",
    "                                                                                     '1900-01-01')\\\n",
    "                                                           .otherwise(sf.concat(sf.col('Anio'),sf.lit('-01-01'))))\n",
    "        df_nuevos_registros=df_nuevos_registros.withColumn(\"FechaFinVigencia\",lit(\"2300-01-01\"))\n",
    "        df_nuevos_registros=df_nuevos_registros.withColumn(\"VersionDelRegistro\",lit(\"S\"))\n",
    "        df_nuevos_registros = df_nuevos_registros.withColumn('IDPIB',  mid() + max_key+1)\n",
    "        #unir en un solo dataframe\n",
    "        df_mantener.createOrReplaceTempView(\"df_mantener\")\n",
    "        df_mantener = spark.sql(\"SELECT INT(IDPIB) ,STRING(Departamento),INT(PIB), DATE(FechaInicioVigencia),\\\n",
    "                       DATE(FechaFinVigencia), STRING(VersionDelRegistro),INT(Anio) from df_mantener\")\n",
    "        \n",
    "        df_actualizar_registrosviejos.createOrReplaceTempView(\"df_actualizar_registrosviejos\")\n",
    "        df_actualizar_registrosviejos = spark.sql(\"SELECT INT(IDPIB) ,STRING(Departamento),INT(PIB), DATE(FechaInicioVigencia),\\\n",
    "                       DATE(FechaFinVigencia), STRING(VersionDelRegistro),INT(Anio) from df_actualizar_registrosviejos\")\n",
    "        \n",
    "        df_actualizar_registrosvigentes.createOrReplaceTempView(\"df_actualizar_registrosvigentes\")\n",
    "        df_actualizar_registrosvigentes = spark.sql(\"SELECT INT(IDPIB) ,STRING(Departamento),INT(PIB), DATE(FechaInicioVigencia),\\\n",
    "                       DATE(FechaFinVigencia), STRING(VersionDelRegistro),INT(Anio) from df_actualizar_registrosvigentes\")\n",
    "        \n",
    "        df_nuevos_registros.createOrReplaceTempView(\"df_nuevos_registros\")\n",
    "        df_nuevos_registros = spark.sql(\"SELECT INT(IDPIB) ,STRING(Departamento),INT(PIB), DATE(FechaInicioVigencia),\\\n",
    "                       DATE(FechaFinVigencia), STRING(VersionDelRegistro),INT(Anio) from df_nuevos_registros\")\n",
    "        \n",
    "        df2 = df_nuevos_registros.union(df_mantener)\n",
    "        df2 = df2.union(df_actualizar_registrosviejos)\n",
    "        df2 = df2.union(df_actualizar_registrosvigentes)\n",
    "        x=df2.count()\n",
    "        df2=write_table(spark, server, database, 'DimInformacionPIB', user,password,'overwrite',df2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23fd0ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=actualizar_historia_PIB(dfBIPparte1)\n",
    "x=actualizar_historia_PIB(dfBIPparte2)\n",
    "x=actualizar_historia_PIB(dfBIPparte3)\n",
    "x=actualizar_historia_PIB(dfBIPparte4)\n",
    "x=actualizar_historia_PIB(dfBIPparte5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d5d939a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-------+\n",
      "|Anio|Mes|IDFecha|\n",
      "+----+---+-------+\n",
      "|2010|  1|      0|\n",
      "|2010|  2|      1|\n",
      "|2010|  3|      2|\n",
      "|2010|  4|      3|\n",
      "|2010|  5|      4|\n",
      "+----+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creacion tabla DimFechaMes\n",
    "df_fechames=dfvuelos.selectExpr('ano as Anio', 'mes as Mes')\n",
    "df_fechames=df_fechames.dropDuplicates()\n",
    "df_fechames=df_fechames.sort(col(\"Anio\"),col('Mes'))\n",
    "df_fechames = df_fechames.coalesce(1).withColumn(\"IDFecha\", mid())\n",
    "df_fechames=write_table(spark, server, database, 'DimFecha', user,password,'overwrite',df_fechames)\n",
    "df_fechames.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "631203aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+\n",
      "|CodigoVuelo|  TipoVuelo|IDTipoVuelo|\n",
      "+-----------+-----------+-----------+\n",
      "|          A|Adicionales|          0|\n",
      "|          C|    Charter|          1|\n",
      "|          R|    Regular|          2|\n",
      "|          T|       Taxi|          3|\n",
      "+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creacion tabla DimTipoVuelo\n",
    "df_tipo_vuelo=dfvuelos.selectExpr('tipo_vuelo as CodigoVuelo')\n",
    "df_tipo_vuelo=df_tipo_vuelo.dropDuplicates()\n",
    "df_tipo_vuelo=df_tipo_vuelo.withColumn(\"TipoVuelo\", \\\n",
    "    when((df_tipo_vuelo.CodigoVuelo ==\"A\"), \"Adicionales\") \\\n",
    "    .when((df_tipo_vuelo.CodigoVuelo ==\"C\"),\"Charter\") \\\n",
    "    .when((df_tipo_vuelo.CodigoVuelo ==\"R\"),\"Regular\") \\\n",
    "    .when((df_tipo_vuelo.CodigoVuelo ==\"T\"),\"Taxi\") \\\n",
    "    .otherwise(\"nan\") \\\n",
    "   )\n",
    "df_tipo_vuelo=df_tipo_vuelo.sort(col(\"CodigoVuelo\"))\n",
    "df_tipo_vuelo = df_tipo_vuelo.coalesce(1).withColumn(\"IDTipoVuelo\", mid())\n",
    "df_tipo_vuelo=write_table(spark, server, database, 'DimTipoVuelo', user,password,'overwrite',df_tipo_vuelo)\n",
    "df_tipo_vuelo.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc35c8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+-------------+\n",
      "|Codigo_Trafico|  Descripcion|IDTipoTrafico|\n",
      "+--------------+-------------+-------------+\n",
      "|             I|Internacional|            0|\n",
      "|             N|     Nacional|            1|\n",
      "+--------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creacion tabla DimTipo_Trafico\n",
    "df_trafico=dfvuelos.selectExpr('trafico as Codigo_Trafico')\n",
    "df_trafico=df_trafico.dropDuplicates()\n",
    "df_trafico=df_trafico.withColumn(\"Descripcion\", \\\n",
    "   when((df_trafico.Codigo_Trafico ==\"I\"), \"Internacional\") \\\n",
    "   .when((df_trafico.Codigo_Trafico ==\"N\"),\"Nacional\") \\\n",
    "   .when((df_trafico.Codigo_Trafico ==\"E\"),\"Externo\") \\\n",
    "   .otherwise(\"nan\") \\\n",
    "  )\n",
    "df_trafico=df_trafico.sort(col(\"Codigo_Trafico\"))\n",
    "df_trafico = df_trafico.coalesce(1).withColumn(\"IDTipoTrafico\", mid())\n",
    "df_trafico=write_table(spark, server, database, 'DimTipo_Trafico', user,password,'overwrite',df_trafico)\n",
    "df_trafico.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdddb66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|       NombreEmpresa|IDEmpresa|\n",
      "+--------------------+---------+\n",
      "|\"SERVICIO AÉREO R...|        0|\n",
      "|              21 AIR|        1|\n",
      "|                ABSA|        2|\n",
      "|ABX AIR INC SUCUR...|        3|\n",
      "| AER CARIBE LIMITADA|        4|\n",
      "+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creacion tabla DimEmpresaTransportadora\n",
    "df_empresatrans1=dfvuelos.selectExpr('empresa as NombreEmpresa')\n",
    "df_empresatrans1=df_empresatrans1.dropDuplicates()\n",
    "df_empresatrans1=df_empresatrans1.sort(col(\"NombreEmpresa\"))\n",
    "df_empresatrans1 = df_empresatrans1.coalesce(1).withColumn(\"IDEmpresa\", mid())\n",
    "df_empresatrans1=write_table(spark, server, database, 'DimEmpresaTransportadora', user,password,'overwrite',df_empresatrans1)\n",
    "df_empresatrans1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c81e9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|NombreEquipo|IDEquipo|\n",
      "+------------+--------+\n",
      "|         318|       0|\n",
      "|         319|       1|\n",
      "|         330|       2|\n",
      "|         332|       3|\n",
      "|         727|       4|\n",
      "+------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creacion tabla DimTipo_Equipo\n",
    "df_tipoequipo=dfvuelos.selectExpr('tipo_equipo as NombreEquipo')\n",
    "df_tipoequipo=df_tipoequipo.dropDuplicates()\n",
    "df_tipoequipo=df_tipoequipo.sort(col(\"NombreEquipo\"))\n",
    "df_tipoequipo = df_tipoequipo.coalesce(1).withColumn(\"IDEquipo\", mid())\n",
    "df_tipoequipo=write_table(spark, server, database, 'DimTipo_Equipo', user,password,'overwrite',df_tipoequipo)\n",
    "df_tipoequipo.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c234cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfaeropuertosconhistoriaparte0 = dfaeropuertos.filter(dfaeropuertos.Ano==\"2013\")\n",
    "dfaeropuertosconhistoriaparte1 = dfaeropuertos.filter(dfaeropuertos.Ano==\"2014\")\n",
    "dfaeropuertosconhistoriaparte2 = dfaeropuertos.filter(dfaeropuertos.Ano==\"2015\")\n",
    "dfaeropuertosconhistoriaparte3 = dfaeropuertos.filter(dfaeropuertos.Ano==\"2016\")\n",
    "dfaeropuertosconhistoriaparte4 = dfaeropuertos.filter(dfaeropuertos.Ano==\"2017\")\n",
    "dfaeropuertosconhistoriaparte5 = dfaeropuertos.filter(dfaeropuertos.Ano==\"2018\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6801b461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actualización tabla DimAeropuertoHistoria\n",
    "from pyspark.sql.types import DateType\n",
    "def actualizar_historia_aeropuertos(df_a_cargar):\n",
    "    InfoPIB = load_table(spark, server, database, 'DimInformacionPIB', user, password)\n",
    "    tablaDWH = load_table(spark, server, database, 'DimAeropuerto', user, password)\n",
    "    df=df_a_cargar.selectExpr('sigla as Sigla', 'iata as IATA', 'nombre as NombreAeropuerto','ubicacion as Ubicacion',\n",
    "                                          'municipio as Municipio','departamento as Departamento', 'categoria as Categoria',\n",
    "                                           'latitud as Latitud','longitud as Longitud', 'propietario as Propietario',\n",
    "                                           'explotador as Explotador','longitud_pista as LongitudPista',\n",
    "                                           'ancho_pista as AnchoPista', 'pbmo as PBMO', 'elevacion as Elevacion',\n",
    "                                           'resolucion as Resolucion','clase as Clase', 'tipo as Tipo', 'pais as Pais',\n",
    "                                           'gcd_municipio as GCD_Municipio', 'gcd_departamento as GCD_Departamento',\n",
    "                                           'Ano as Anio')\n",
    "    if tablaDWH.count()==0:\n",
    "        \n",
    "        df=df.withColumn(\"FechaInicioVigencia\",lit(\"1900-01-01\"))\n",
    "        df=df.withColumn(\"FechaFinVigencia\",lit(\"2300-01-01\"))\n",
    "        df=df.withColumn(\"VersionDelRegistro\",lit(\"S\"))\n",
    "        df=df.dropDuplicates()\n",
    "        df=df.sort(col(\"Sigla\"))\n",
    "        df = df.coalesce(1).withColumn(\"IDAeropuerto\", mid())\n",
    "        df.createOrReplaceTempView(\"df\")\n",
    "        InfoPIB.createOrReplaceTempView(\"InfoPIB\")\n",
    "        df = spark.sql(\"SELECT INT(df.IDAeropuerto), STRING(df.Sigla), STRING(df.IATA), STRING(df.NombreAeropuerto),\\\n",
    "                        STRING(df.Ubicacion),string(df.Pais),\\\n",
    "                        STRING(df.Categoria),DOUBLE(df.Latitud), DOUBLE(df.Longitud),STRING(df.Municipio), STRING(df.Departamento), \\\n",
    "                         STRING(df.Propietario),STRING(df.Explotador),INT(df.LongitudPista), INT(df.AnchoPista),\\\n",
    "                        STRING(df.PBMO),INT(df.Elevacion), STRING(df.Resolucion), STRING(df.Clase),\\\n",
    "                        STRING(df.Tipo),STRING(df.GCD_Municipio), STRING(df.GCD_Departamento), DATE(df.FechaInicioVigencia),\\\n",
    "                       DATE(df.FechaFinVigencia), STRING(df.VersionDelRegistro),INT(df.Anio), INT(InfoPIB.IDPIB) from df\\\n",
    "                       left join InfoPIB on InfoPIB.Departamento= df.Departamento and\\\n",
    "                             InfoPIB.Anio= df.Anio\")\n",
    "        \n",
    "        x=df.count()\n",
    "        df=write_table(spark, server, database, 'DimAeropuerto', user,password,'overwrite',df)\n",
    "    else:\n",
    "        tablaDWH.persist()\n",
    "        df_fechas_antiguas=tablaDWH.selectExpr('Sigla','Sigla as SiglaAnt')\n",
    "        df_fechas_antiguas=df_fechas_antiguas.dropDuplicates()\n",
    "        df_nuevo=df.selectExpr('Sigla','Anio as AnioNuevo')\n",
    "        df_nuevo=df_nuevo.dropDuplicates()\n",
    "        df_temp=tablaDWH.join(df_nuevo, how = 'left', on = 'Sigla')\n",
    "        #registros que no tienen actualizacion\n",
    "        df_temp.persist()\n",
    "        df_mantener=df_temp.filter(df_temp.AnioNuevo.isNull())\n",
    "        df_mantener=df_mantener.drop('AnioNuevo')\n",
    "        df_mantener=df_mantener.withColumn(\"Origen\",lit(\"Mantener\"))\n",
    "        #registros que si tienen actualización\n",
    "        df_actualizar=df_temp.filter(df_temp.AnioNuevo.isNotNull())\n",
    "        #registros viejos que no van a cambiar\n",
    "        df_actualizar.persist()\n",
    "        df_actualizar_registrosviejos=df_actualizar.filter(df_actualizar.VersionDelRegistro==\"N\")\n",
    "        df_actualizar_registrosviejos=df_actualizar_registrosviejos.drop('AnioNuevo')\n",
    "        #actualización de registros que eran vigentes\n",
    "        df_actualizar_registrosvigentes=df_actualizar.filter(df_actualizar.VersionDelRegistro==\"S\")\n",
    "        df_actualizar_registrosvigentes=df_actualizar_registrosvigentes.withColumn(\"VersionDelRegistro\",lit(\"N\"))\n",
    "        df_actualizar_registrosvigentes=df_actualizar_registrosvigentes.withColumn(\"FechaFinVigencia\",\n",
    "                                                                                    sf.concat(sf.col('Anio'),sf.lit('-12-31'))\n",
    "                                                                                   )    \n",
    "        df_actualizar_registrosvigentes=df_actualizar_registrosvigentes.drop('AnioNuevo')\n",
    "        #registros nuevos para ingresar a la base\n",
    "        #Encontar llave máxima\n",
    "        max_key = tablaDWH.agg({\"IDAeropuerto\": \"max\"}).collect()[0][0]\n",
    "        df_nuevos_registros=df.alias('df_nuevos_registros')\n",
    "        df_nuevos_registros=df_nuevos_registros.join(df_fechas_antiguas,how = 'left', on = 'Sigla')\n",
    "        df_nuevos_registros=df_nuevos_registros.withColumn(\"FechaInicioVigencia\",when(df_nuevos_registros.SiglaAnt.isNull(),\\\n",
    "                                                                                     '1900-01-01')\\\n",
    "                                                           .otherwise(sf.concat(sf.col('Anio'),sf.lit('-01-01'))))\n",
    "        df_nuevos_registros=df_nuevos_registros.withColumn(\"FechaFinVigencia\",lit(\"2300-01-01\"))\n",
    "        df_nuevos_registros=df_nuevos_registros.withColumn(\"VersionDelRegistro\",lit(\"S\"))\n",
    "        df_nuevos_registros = df_nuevos_registros.withColumn('IDAeropuerto',  mid() + max_key+1)\n",
    "        #unir en un solo dataframe\n",
    "        InfoPIB.createOrReplaceTempView(\"InfoPIB\")\n",
    "        df_mantener.createOrReplaceTempView(\"df_mantener\")\n",
    "        df_mantener = spark.sql(\"SELECT INT(df_mantener.IDAeropuerto), STRING(df_mantener.Sigla), STRING(df_mantener.IATA), STRING(df_mantener.NombreAeropuerto),\\\n",
    "                        STRING(df_mantener.Categoria),DOUBLE(df_mantener.Latitud), DOUBLE(df_mantener.Longitud),STRING(df_mantener.Municipio), STRING(df_mantener.Departamento), \\\n",
    "                        STRING(df_mantener.Propietario),STRING(df_mantener.Explotador),INT(df_mantener.LongitudPista), INT(df_mantener.AnchoPista),\\\n",
    "                        STRING(df_mantener.PBMO),INT(df_mantener.Elevacion), STRING(df_mantener.Resolucion), STRING(df_mantener.Clase),\\\n",
    "                        STRING(df_mantener.Ubicacion),string(df_mantener.Pais),\\\n",
    "                        STRING(df_mantener.Tipo),STRING(df_mantener.GCD_Municipio), STRING(df_mantener.GCD_Departamento), DATE(df_mantener.FechaInicioVigencia),\\\n",
    "                        DATE(df_mantener.FechaFinVigencia), STRING(df_mantener.VersionDelRegistro),INT(df_mantener.Anio), INT(InfoPIB.IDPIB) from df_mantener\\\n",
    "                       left join InfoPIB on InfoPIB.Departamento= df_mantener.Departamento and\\\n",
    "                             InfoPIB.Anio= df_mantener.Anio\")\n",
    "        \n",
    "        df_actualizar_registrosviejos.createOrReplaceTempView(\"df_actualizar_registrosviejos\")\n",
    "        df_actualizar_registrosviejos = spark.sql(\"SELECT INT(df_actualizar_registrosviejos.IDAeropuerto), STRING(df_actualizar_registrosviejos.Sigla), STRING(df_actualizar_registrosviejos.IATA), STRING(df_actualizar_registrosviejos.NombreAeropuerto),\\\n",
    "                        STRING(df_actualizar_registrosviejos.Categoria),DOUBLE(df_actualizar_registrosviejos.Latitud), DOUBLE(df_actualizar_registrosviejos.Longitud),STRING(df_actualizar_registrosviejos.Municipio), STRING(df_actualizar_registrosviejos.Departamento), \\\n",
    "                        STRING(df_actualizar_registrosviejos.Propietario),STRING(df_actualizar_registrosviejos.Explotador),INT(df_actualizar_registrosviejos.LongitudPista), INT(df_actualizar_registrosviejos.AnchoPista),\\\n",
    "                        STRING(df_actualizar_registrosviejos.PBMO),INT(df_actualizar_registrosviejos.Elevacion), STRING(df_actualizar_registrosviejos.Resolucion), STRING(df_actualizar_registrosviejos.Clase),\\\n",
    "                        STRING(df_actualizar_registrosviejos.Ubicacion),string(df_actualizar_registrosviejos.Pais),\\\n",
    "                        STRING(df_actualizar_registrosviejos.Tipo),STRING(df_actualizar_registrosviejos.GCD_Municipio), STRING(df_actualizar_registrosviejos.GCD_Departamento), DATE(df_actualizar_registrosviejos.FechaInicioVigencia),\\\n",
    "                        DATE(df_actualizar_registrosviejos.FechaFinVigencia),\\\n",
    "                        STRING(df_actualizar_registrosviejos.VersionDelRegistro),INT(df_actualizar_registrosviejos.Anio), INT(InfoPIB.IDPIB) from df_actualizar_registrosviejos\\\n",
    "                       left join InfoPIB on InfoPIB.Departamento= df_actualizar_registrosviejos.Departamento and\\\n",
    "                             InfoPIB.Anio= df_actualizar_registrosviejos.Anio\")\n",
    "        \n",
    "        df_actualizar_registrosvigentes.createOrReplaceTempView(\"df_actualizar_registrosvigentes\")\n",
    "        df_actualizar_registrosvigentes = spark.sql(\"SELECT INT(df_actualizar_registrosvigentes.IDAeropuerto), STRING(Sigla), STRING(IATA), STRING(NombreAeropuerto),\\\n",
    "                        STRING(Categoria),DOUBLE(Latitud), DOUBLE(Longitud),STRING(Municipio), STRING(df_actualizar_registrosvigentes.Departamento), \\\n",
    "                         STRING(Propietario),STRING(Explotador),INT(LongitudPista), INT(AnchoPista),\\\n",
    "                        STRING(PBMO),INT(Elevacion), STRING(Resolucion), STRING(Clase),\\\n",
    "                        STRING(Ubicacion),string(Pais),\\\n",
    "                        STRING(Tipo),STRING(GCD_Municipio), STRING(GCD_Departamento), DATE(df_actualizar_registrosvigentes.FechaInicioVigencia),\\\n",
    "                       DATE(df_actualizar_registrosvigentes.FechaFinVigencia), STRING(df_actualizar_registrosvigentes.VersionDelRegistro),\\\n",
    "                       INT(df_actualizar_registrosvigentes.Anio), INT(InfoPIB.IDPIB) from df_actualizar_registrosvigentes\\\n",
    "                       left join InfoPIB on InfoPIB.Departamento= df_actualizar_registrosvigentes.Departamento and\\\n",
    "                             InfoPIB.Anio= df_actualizar_registrosvigentes.Anio\")\n",
    "        \n",
    "        df_nuevos_registros.createOrReplaceTempView(\"df_nuevos_registros\")\n",
    "        df_nuevos_registros = spark.sql(\"SELECT INT(df_nuevos_registros.IDAeropuerto), STRING(df_nuevos_registros.Sigla), STRING(df_nuevos_registros.IATA), STRING(df_nuevos_registros.NombreAeropuerto),\\\n",
    "                        STRING(df_nuevos_registros.Categoria),DOUBLE(df_nuevos_registros.Latitud), DOUBLE(df_nuevos_registros.Longitud),STRING(df_nuevos_registros.Municipio), STRING(df_nuevos_registros.Departamento), \\\n",
    "                         STRING(df_nuevos_registros.Propietario),STRING(df_nuevos_registros.Explotador),INT(df_nuevos_registros.LongitudPista), INT(df_nuevos_registros.AnchoPista),\\\n",
    "                        STRING(df_nuevos_registros.PBMO),INT(df_nuevos_registros.Elevacion), STRING(df_nuevos_registros.Resolucion), STRING(df_nuevos_registros.Clase),\\\n",
    "                        STRING(df_nuevos_registros.Ubicacion),string(df_nuevos_registros.Pais),\\\n",
    "                        STRING(df_nuevos_registros.Tipo),STRING(df_nuevos_registros.GCD_Municipio), STRING(df_nuevos_registros.GCD_Departamento), DATE(df_nuevos_registros.FechaInicioVigencia),\\\n",
    "                       DATE(df_nuevos_registros.FechaFinVigencia), STRING(df_nuevos_registros.VersionDelRegistro),\\\n",
    "                       INT(df_nuevos_registros.Anio), INT(InfoPIB.IDPIB) from df_nuevos_registros\\\n",
    "                       left join InfoPIB on InfoPIB.Departamento= df_nuevos_registros.Departamento and\\\n",
    "                             InfoPIB.Anio= df_nuevos_registros.Anio\")\n",
    "        \n",
    "        df2 = df_nuevos_registros.union(df_mantener)\n",
    "        df2 = df2.union(df_actualizar_registrosviejos)\n",
    "        df2 = df2.union(df_actualizar_registrosvigentes)\n",
    "        \n",
    "        x=df2.count()\n",
    "        df2=write_table(spark, server, database, 'DimAeropuerto', user,password,'overwrite',df2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec768c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=actualizar_historia_aeropuertos(dfaeropuertosconhistoriaparte0)\n",
    "x=actualizar_historia_aeropuertos(dfaeropuertosconhistoriaparte1)\n",
    "x=actualizar_historia_aeropuertos(dfaeropuertosconhistoriaparte2)\n",
    "x=actualizar_historia_aeropuertos(dfaeropuertosconhistoriaparte3)\n",
    "x=actualizar_historia_aeropuertos(dfaeropuertosconhistoriaparte4)\n",
    "x=actualizar_historia_aeropuertos(dfaeropuertosconhistoriaparte5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "737cdd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creacion Tabla de Hechos Vuelos\n",
    "import pyspark.sql.functions as F\n",
    "DimAeropuerto = load_table(spark, server, database, 'DimAeropuerto', user, password)\n",
    "df_aeropuertoorigen=DimAeropuerto.selectExpr('Sigla as origen','Anio','FechaInicioVigencia',\n",
    "                                                     'FechaFinVigencia','IDAeropuerto as IDAeropuertoOrigen')\n",
    "df_aeropuertodestino=DimAeropuerto.selectExpr('Sigla as destino','Anio','FechaInicioVigencia',\n",
    "                                                      'FechaFinVigencia','IDAeropuerto as IDAeropuertoDestino')\n",
    "\n",
    "df_hechos_vuelos=dfvuelos.alias('df_hechos_vuelos')\n",
    "columns = ['vuelos', 'sillas','pasajeros','carga_bordo','carga_ofrecida']\n",
    "for column in columns:\n",
    "    df_hechos_vuelos = df_hechos_vuelos.withColumn(column,F.when(F.isnan(F.col(column)),0).otherwise(F.col(column)))\n",
    "\n",
    "df_hechos_vuelos=df_hechos_vuelos.filter(df_hechos_vuelos.origen!=df_hechos_vuelos.destino)\n",
    "df_hechos_vuelos= df_hechos_vuelos.withColumn(\"vuelos\",df_hechos_vuelos.vuelos.cast('int'))\n",
    "df_hechos_vuelos= df_hechos_vuelos.withColumn(\"vuelos\",df_hechos_vuelos.vuelos.cast('int'))\n",
    "df_hechos_vuelos= df_hechos_vuelos.withColumn(\"relativedate\",sf.concat(sf.col('ano'),lit(\"-\"),sf.col('mes'),sf.lit('-01')))\n",
    "df_hechos_vuelos= df_hechos_vuelos.withColumn(\"vuelos\",df_hechos_vuelos.vuelos.cast('int'))\n",
    "df_hechos_vuelos= df_hechos_vuelos.withColumn(\"sillas\",df_hechos_vuelos.sillas.cast('int'))\n",
    "df_hechos_vuelos= df_hechos_vuelos.withColumn(\"carga_ofrecida\",df_hechos_vuelos.carga_ofrecida.cast('int'))\n",
    "df_hechos_vuelos= df_hechos_vuelos.withColumn(\"carga_bordo\",df_hechos_vuelos.carga_bordo.cast('int'))\n",
    "df_hechos_vuelos= df_hechos_vuelos.withColumn(\"pasajeros\",df_hechos_vuelos.pasajeros.cast('int'))\n",
    "df_hechos_vuelos=df_hechos_vuelos.filter(df_hechos_vuelos.vuelos!=0)\n",
    "df_hechos_vuelos=df_hechos_vuelos.withColumn(\"sillas\",when(df_hechos_vuelos.sillas == 0,df_hechos_vuelos.pasajeros).otherwise(df_hechos_vuelos.sillas))\n",
    "df_hechos_vuelos=df_hechos_vuelos.withColumn(\"carga_ofrecida\",when(df_hechos_vuelos.carga_ofrecida == 0,df_hechos_vuelos.carga_bordo).otherwise(df_hechos_vuelos.carga_ofrecida))\n",
    "df_hechos_vuelos=df_hechos_vuelos.groupBy(\"relativedate\",\"ano\",\"mes\",\"origen\",\"destino\",\"tipo_equipo\",\"tipo_vuelo\",\"trafico\",\"empresa\") \\\n",
    "    .sum(\"vuelos\",\"pasajeros\",\"carga_bordo\",\"sillas\",\"carga_ofrecida\")\n",
    "df_hechos_vuelos=df_hechos_vuelos.withColumnRenamed(\"sum(vuelos)\", \"Vuelos\")\n",
    "df_hechos_vuelos=df_hechos_vuelos.withColumnRenamed(\"sum(pasajeros)\", \"Pasajeros\")\n",
    "df_hechos_vuelos=df_hechos_vuelos.withColumnRenamed(\"sum(carga_bordo)\", \"CargaBordo\")\n",
    "df_hechos_vuelos=df_hechos_vuelos.withColumnRenamed(\"sum(sillas)\", \"TotalSillas\")\n",
    "df_hechos_vuelos=df_hechos_vuelos.withColumnRenamed(\"sum(carga_ofrecida)\", \"TotalCarga\")\n",
    "df_hechos_vuelos.createOrReplaceTempView(\"df_hechos_vuelos\")\n",
    "df_aeropuertoorigen.createOrReplaceTempView(\"df_aeropuertoorigen\")\n",
    "df_aeropuertodestino.createOrReplaceTempView(\"df_aeropuertodestino\")\n",
    "df_fechames.createOrReplaceTempView(\"df_fechames\")\n",
    "df_tipo_vuelo.createOrReplaceTempView(\"df_tipo_vuelo\")\n",
    "df_empresatrans1.createOrReplaceTempView(\"df_empresatrans1\")\n",
    "df_trafico.createOrReplaceTempView(\"df_trafico\")\n",
    "df_tipoequipo.createOrReplaceTempView(\"df_tipoequipo\")\n",
    "df_hechos_vuelos = spark.sql(\"select  IDFecha,IDTipoVuelo,IDTipoTrafico,IDEmpresa,IDEquipo,IDAeropuertoOrigen,\\\n",
    "                             IDAeropuertoDestino,Vuelos,Pasajeros,CargaBordo,TotalSillas,TotalCarga from df_hechos_vuelos\\\n",
    "                             left join df_fechames on df_fechames.Anio= df_hechos_vuelos.ano and\\\n",
    "                             df_fechames.Mes= df_hechos_vuelos.mes\\\n",
    "                             left join df_tipo_vuelo on df_tipo_vuelo.CodigoVuelo= df_hechos_vuelos.tipo_vuelo\\\n",
    "                             left join df_trafico on df_trafico.Codigo_Trafico= df_hechos_vuelos.trafico\\\n",
    "                             left join df_empresatrans1 on df_empresatrans1.NombreEmpresa= df_hechos_vuelos.empresa\\\n",
    "                             left join df_tipoequipo on df_tipoequipo.NombreEquipo= df_hechos_vuelos.tipo_equipo\\\n",
    "                             left join df_aeropuertoorigen on df_aeropuertoorigen.origen= df_hechos_vuelos.origen and\\\n",
    "                             (df_hechos_vuelos.relativedate BETWEEN df_aeropuertoorigen.FechaInicioVigencia\\\n",
    "                             AND df_aeropuertoorigen.FechaFinVigencia)\\\n",
    "                             left join df_aeropuertodestino on df_aeropuertodestino.destino= df_hechos_vuelos.destino and\\\n",
    "                             (df_hechos_vuelos.relativedate BETWEEN df_aeropuertodestino.FechaInicioVigencia\\\n",
    "                             AND df_aeropuertodestino.FechaFinVigencia)\")\n",
    "df_hechos_vuelos = df_hechos_vuelos.coalesce(1).withColumn(\"ID\", mid())\n",
    "df_hechos_vuelos=df_hechos_vuelos.filter(df_hechos_vuelos.IDAeropuertoOrigen.isNotNull())\n",
    "df_hechos_vuelos=df_hechos_vuelos.filter(df_hechos_vuelos.IDAeropuertoDestino.isNotNull())\n",
    "df_hechos_vuelos=write_table(spark, server, database, 'FactVuelos', user,password,'overwrite',df_hechos_vuelos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea382341",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d57aa50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
