{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c5b411f-b096-4a7c-a2b7-828ac869443b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql import functions as f\n",
    "from pymssql import _mssql\n",
    "from pyspark.sql.functions import monotonically_increasing_id as mid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a9c874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "appName = \"PySpark SQL Server via JDBC\"\n",
    "master = \"local\"\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(appName) \\\n",
    "    .setMaster(master) \\\n",
    "    .set(\"spark.driver.extraClassPath\",\"C:\\\\Users\\\\estudiante\\\\mssql-jdbc-9.2.1.jre8.jar\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sql_context = SQLContext(sc)\n",
    "spark = sql_context.sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c883752f",
   "metadata": {},
   "outputs": [],
   "source": [
    "server='localhost:1433'\n",
    "database = \"ProyectoAeropuertosFinal\"\n",
    "user = \"sa\"\n",
    "password  = \"12345\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e3f4b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = _mssql.connect(server=server, user=user, password=password,database=database)\n",
    "query = f\"-- Create schemas\\\n",
    "-- Create tables\\\n",
    "IF (NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = 'DimFecha'))\\\n",
    "BEGIN\\\n",
    "  CREATE TABLE DimFecha\\\n",
    "  (\\\n",
    "    IDFecha INT NOT NULL,\\\n",
    "    Año VARCHAR(4),\\\n",
    "    Mes VARCHAR(2),\\\n",
    "    PRIMARY KEY(IDFecha)\\\n",
    "  )\\\n",
    "END;\\\n",
    "\\\n",
    "IF (NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = 'DimTipo_Equipo'))\\\n",
    "BEGIN\\\n",
    "  CREATE TABLE DimTipo_Equipo\\\n",
    "  (\\\n",
    "    IDEquipo INT NOT NULL,\\\n",
    "    NombreEquipo VARCHAR(4),\\\n",
    "    PRIMARY KEY(IDEquipo)\\\n",
    "  )\\\n",
    "END;\\\n",
    "\\\n",
    "IF (NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = 'DimTipoVuelo'))\\\n",
    "BEGIN\\\n",
    "  CREATE TABLE DimTipoVuelo\\\n",
    "  (\\\n",
    "    IDTipoVuelo INT NOT NULL,\\\n",
    "    CodigoVuelo VARCHAR(1),\\\n",
    "    TipoVuelo VARCHAR(10),\\\n",
    "    PRIMARY KEY(IDTipoVuelo)\\\n",
    "  )\\\n",
    "END;\\\n",
    "\\\n",
    "IF (NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = 'DimTipo_Trafico'))\\\n",
    "BEGIN\\\n",
    "  CREATE TABLE DimTipo_Trafico\\\n",
    "  (\\\n",
    "    IDTipoTrafico INT NOT NULL,\\\n",
    "    Codigo_Trafico VARCHAR(1),\\\n",
    "    Descripcion VARCHAR(10),\\\n",
    "    PRIMARY KEY(IDTipoTrafico)\\\n",
    "  )\\\n",
    "END;\\\n",
    "\\\n",
    "IF (NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = 'DimEmpresaTransportadora'))\\\n",
    "BEGIN\\\n",
    "  CREATE TABLE DimEmpresaTransportadora\\\n",
    "  (\\\n",
    "    IDEmpresa INT NOT NULL,\\\n",
    "    NombreEmpresa VARCHAR(50),\\\n",
    "    PRIMARY KEY(IDEmpresa)\\\n",
    "  )\\\n",
    "END;\\\n",
    "\\\n",
    "IF (NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = 'FactVuelos'))\\\n",
    "BEGIN\\\n",
    "  CREATE TABLE FactVuelos\\\n",
    "  (\\\n",
    "    ID INT NOT NULL,\\\n",
    "    IDFecha INT,\\\n",
    "    IDTipoEquipo INT,\\\n",
    "    IDAeropuertoOrigen INT,\\\n",
    "    IDAeropuertoDestino INT,\\\n",
    "    IDTipoVuelo INT,\\\n",
    "    IDTipoTrafico INT,\\\n",
    "    IDEmpresa INT,\\\n",
    "    Vuelos INT,\\\n",
    "    Pasajeros INT,\\\n",
    "    CargaBordo INT,\\\n",
    "    TotalSillas INT,\\\n",
    "    TotalCarga INT,\\\n",
    "    PRIMARY KEY(ID)\\\n",
    "  )\\\n",
    "END;\\\n",
    "\\\n",
    "IF (NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = 'DimAeropuertoHistoria'))\\\n",
    "BEGIN\\\n",
    "  CREATE TABLE DimAeropuerto\\\n",
    "  (\\\n",
    "    IDAeropuerto INT NOT NULL,\\\n",
    "    Sigla CHARACTER(3),\\\n",
    "    IATA CHARACTER(3),\\\n",
    "    Ubicacion VARCHAR(50),\\\n",
    "    NombreAeropuerto VARCHAR(50),\\\n",
    "    Municipio VARCHAR(50),\\\n",
    "    Departamento VARCHAR(50),\\\n",
    "    Pais VARCHAR(50),\\\n",
    "    Categoria VARCHAR(50),\\\n",
    "    Latitud FLOAT,\\\n",
    "    Longitud FLOAT,\\\n",
    "    Propietario VARCHAR(50),\\\n",
    "    Explotador VARCHAR(50),\\\n",
    "    LongitudPista INT,\\\n",
    "    AnchoPista INT,\\\n",
    "    PBMO INT,\\\n",
    "    Elevacion INT,\\\n",
    "    Resolucion VARCHAR(50),\\\n",
    "    Clase VARCHAR(50),\\\n",
    "    Tipo VARCHAR(50),\\\n",
    "    GCD_Municipio VARCHAR(50),\\\n",
    "    GCD_Departamento VARCHAR(50),\\\n",
    "    FechaInicioVigencia DATE,\\\n",
    "    FechaFinVigencia DATE,\\\n",
    "    VersionDelRegistro VARCHAR(1),\\\n",
    "    Anio INT,\\\n",
    "    PRIMARY KEY(IDAeropuerto)\\\n",
    "  )\\\n",
    "END;\\\n",
    "\"\n",
    "conn.execute_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc84d956",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfvuelos = spark.read.format(\"csv\").load(\"vuelosEtapa3.csv\",format=\"csv\",sep=\",\",\n",
    "                                         inferSchema='true',header='true')\n",
    "dfaeropuertos =spark.read.format(\"csv\").load(\"aeropuertosEtapa3.csv\",format=\"csv\",sep=\";\",\n",
    "                                         inferSchema='true',header='true')\n",
    "dfaeropuertosdelmundo=spark.read.format(\"csv\").load(\"Aeropuertosdelmundo.csv\",format=\"csv\",sep=\";\",\n",
    "                                         inferSchema='true',header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dd631d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sequence, to_date, explode, col,when,lit,expr,substring,regexp_replace\n",
    "from pyspark.sql import functions as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db279f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agregar nombres de los aeropuertos internacionales\n",
    "dfaeropuertosdelmundo=dfaeropuertosdelmundo.dropDuplicates()\n",
    "dfaeropuertosdelmundo_origen=dfaeropuertosdelmundo.selectExpr('Origen as sigla', 'Ciudad_Origen as municipio',\n",
    "                                                              'APTO_ORIGEN as nombre', 'Pais_Origen as pais')\n",
    "dfaeropuertosdelmundo_destino=dfaeropuertosdelmundo.selectExpr('Destino as sigla', 'Ciudad_Destino as municipio',\n",
    "                                                              'APTO_DESTINO as nombre', 'Pais_Destino as pais')\n",
    "dfaeropuertosdelmundo = dfaeropuertosdelmundo_origen.union(dfaeropuertosdelmundo_destino).where(\"pais is not null\") \n",
    "dfaeropuertosdelmundo=dfaeropuertosdelmundo.dropDuplicates().filter(dfaeropuertosdelmundo.pais!=\"COLOMBIA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e03a93b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfaeropuertos=dfaeropuertos.withColumn(\"pais\",lit(\"COLOMBIA\")).withColumn(\"ubicacion\",lit(\"Nacional\"))\n",
    "dfaeropuertosdelmundo=dfaeropuertosdelmundo.withColumn(\"Ano\",lit(\"2014\")).withColumn(\"ubicacion\",lit(\"Internacional\"))\n",
    "dfaeropuertos=dfaeropuertos.unionByName(dfaeropuertosdelmundo, allowMissingColumns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70bb9b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_table(spark, jdbc_hostname, database, data_table, username, password):\n",
    "    jdbc_url = \"jdbc:sqlserver://{0};database={1}\".format(jdbc_hostname, database)\n",
    "\n",
    "    connection_details = {\n",
    "        \"user\": username,\n",
    "        \"password\": password,\n",
    "        \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n",
    "    }\n",
    "\n",
    "    df = spark.read.jdbc(url=jdbc_url, table=data_table, properties=connection_details)\n",
    "    return df\n",
    "\n",
    "def write_table(spark, jdbc_hostname, database, data_table, username, password,mode,df):\n",
    "    jdbc_url = \"jdbc:sqlserver://{0};database={1}\".format(jdbc_hostname, database)\n",
    "\n",
    "    connection_details = {\n",
    "        \"user\": username,\n",
    "        \"password\": password,\n",
    "        \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n",
    "        \n",
    "        \n",
    "    }\n",
    "\n",
    "    df.write.jdbc(url=jdbc_url, table=data_table, properties=connection_details, mode=mode)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba8c3f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-------+\n",
      "|Anio|Mes|IDFecha|\n",
      "+----+---+-------+\n",
      "|2010|  1|      0|\n",
      "|2010|  2|      1|\n",
      "|2010|  3|      2|\n",
      "|2010|  4|      3|\n",
      "|2010|  5|      4|\n",
      "+----+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creacion tabla DimFechaMes\n",
    "df_fechames=dfvuelos.selectExpr('ano as Anio', 'mes as Mes')\n",
    "df_fechames=df_fechames.dropDuplicates()\n",
    "df_fechames=df_fechames.sort(col(\"Anio\"),col('Mes'))\n",
    "df_fechames = df_fechames.coalesce(1).withColumn(\"IDFecha\", mid())\n",
    "df_fechames=write_table(spark, server, database, 'DimFecha', user,password,'overwrite',df_fechames)\n",
    "df_fechames.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64d30cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-----------+\n",
      "|CodigoVuelo|  TipoVuelo|IDTipoVuelo|\n",
      "+-----------+-----------+-----------+\n",
      "|          A|Adicionales|          0|\n",
      "|          C|    Charter|          1|\n",
      "|          R|    Regular|          2|\n",
      "|          T|       Taxi|          3|\n",
      "+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creacion tabla DimTipoVuelo\n",
    "df_tipo_vuelo=dfvuelos.selectExpr('tipo_vuelo as CodigoVuelo')\n",
    "df_tipo_vuelo=df_tipo_vuelo.dropDuplicates()\n",
    "df_tipo_vuelo=df_tipo_vuelo.withColumn(\"TipoVuelo\", \\\n",
    "    when((df_tipo_vuelo.CodigoVuelo ==\"A\"), \"Adicionales\") \\\n",
    "    .when((df_tipo_vuelo.CodigoVuelo ==\"C\"),\"Charter\") \\\n",
    "    .when((df_tipo_vuelo.CodigoVuelo ==\"R\"),\"Regular\") \\\n",
    "    .when((df_tipo_vuelo.CodigoVuelo ==\"T\"),\"Taxi\") \\\n",
    "    .otherwise(\"nan\") \\\n",
    "   )\n",
    "df_tipo_vuelo=df_tipo_vuelo.sort(col(\"CodigoVuelo\"))\n",
    "df_tipo_vuelo = df_tipo_vuelo.coalesce(1).withColumn(\"IDTipoVuelo\", mid())\n",
    "df_tipo_vuelo=write_table(spark, server, database, 'DimTipoVuelo', user,password,'overwrite',df_tipo_vuelo)\n",
    "df_tipo_vuelo.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5533e417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+-------------+\n",
      "|Codigo_Trafico|  Descripcion|IDTipoTrafico|\n",
      "+--------------+-------------+-------------+\n",
      "|             I|Internacional|            0|\n",
      "|             N|     Nacional|            1|\n",
      "+--------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creacion tabla DimTipo_Trafico\n",
    "df_trafico=dfvuelos.selectExpr('trafico as Codigo_Trafico')\n",
    "df_trafico=df_trafico.dropDuplicates()\n",
    "df_trafico=df_trafico.withColumn(\"Descripcion\", \\\n",
    "   when((df_trafico.Codigo_Trafico ==\"I\"), \"Internacional\") \\\n",
    "   .when((df_trafico.Codigo_Trafico ==\"N\"),\"Nacional\") \\\n",
    "   .when((df_trafico.Codigo_Trafico ==\"E\"),\"Externo\") \\\n",
    "   .otherwise(\"nan\") \\\n",
    "  )\n",
    "df_trafico=df_trafico.sort(col(\"Codigo_Trafico\"))\n",
    "df_trafico = df_trafico.coalesce(1).withColumn(\"IDTipoTrafico\", mid())\n",
    "df_trafico=write_table(spark, server, database, 'DimTipo_Trafico', user,password,'overwrite',df_trafico)\n",
    "df_trafico.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d975d91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|       NombreEmpresa|IDEmpresa|\n",
      "+--------------------+---------+\n",
      "|\"SERVICIO AÉREO R...|        0|\n",
      "|              21 AIR|        1|\n",
      "|                ABSA|        2|\n",
      "|ABX AIR INC SUCUR...|        3|\n",
      "| AER CARIBE LIMITADA|        4|\n",
      "+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creacion tabla DimEmpresaTransportadora\n",
    "df_empresatrans1=dfvuelos.selectExpr('empresa as NombreEmpresa')\n",
    "df_empresatrans1=df_empresatrans1.dropDuplicates()\n",
    "df_empresatrans1=df_empresatrans1.sort(col(\"NombreEmpresa\"))\n",
    "df_empresatrans1 = df_empresatrans1.coalesce(1).withColumn(\"IDEmpresa\", mid())\n",
    "df_empresatrans1=write_table(spark, server, database, 'DimEmpresaTransportadora', user,password,'overwrite',df_empresatrans1)\n",
    "df_empresatrans1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "538fc31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+\n",
      "|NombreEquipo|IDEquipo|\n",
      "+------------+--------+\n",
      "|         318|       0|\n",
      "|         319|       1|\n",
      "|         330|       2|\n",
      "|         332|       3|\n",
      "|         727|       4|\n",
      "+------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creacion tabla DimTipo_Equipo\n",
    "df_tipoequipo=dfvuelos.selectExpr('tipo_equipo as NombreEquipo')\n",
    "df_tipoequipo=df_tipoequipo.dropDuplicates()\n",
    "df_tipoequipo=df_tipoequipo.sort(col(\"NombreEquipo\"))\n",
    "df_tipoequipo = df_tipoequipo.coalesce(1).withColumn(\"IDEquipo\", mid())\n",
    "df_tipoequipo=write_table(spark, server, database, 'DimTipo_Equipo', user,password,'overwrite',df_tipoequipo)\n",
    "df_tipoequipo.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dfbc91cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfaeropuertosconhistoriaparte1 = dfaeropuertos.filter(dfaeropuertos.Ano==\"2014\")\n",
    "dfaeropuertosconhistoriaparte2 = dfaeropuertos.filter(dfaeropuertos.Ano==\"2015\")\n",
    "dfaeropuertosconhistoriaparte3 = dfaeropuertos.filter(dfaeropuertos.Ano==\"2016\")\n",
    "dfaeropuertosconhistoriaparte4 = dfaeropuertos.filter(dfaeropuertos.Ano==\"2017\")\n",
    "dfaeropuertosconhistoriaparte5 = dfaeropuertos.filter(dfaeropuertos.Ano==\"2018\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9b2b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actualización tabla DimAeropuertoHistoria\n",
    "from pyspark.sql.types import DateType\n",
    "def actualizar_historia_aeropuertos(df_a_cargar):\n",
    "    \n",
    "    tablaDWH = load_table(spark, server, database, 'DimAeropuerto', user, password)\n",
    "    df=df_a_cargar.selectExpr('sigla as Sigla', 'iata as IATA', 'nombre as NombreAeropuerto','ubicacion as Ubicacion',\n",
    "                                          'municipio as Municipio','departamento as Departamento', 'categoria as Categoria',\n",
    "                                           'latitud as Latitud','longitud as Longitud', 'propietario as Propietario',\n",
    "                                           'explotador as Explotador','longitud_pista as LongitudPista',\n",
    "                                           'ancho_pista as AnchoPista', 'pbmo as PBMO', 'elevacion as Elevacion',\n",
    "                                           'resolucion as Resolucion','clase as Clase', 'tipo as Tipo', 'pais as Pais',\n",
    "                                           'gcd_municipio as GCD_Municipio', 'gcd_departamento as GCD_Departamento',\n",
    "                                           'Ano as Anio')\n",
    "    if tablaDWH.count()==0:\n",
    "        \n",
    "        df=df.withColumn(\"FechaInicioVigencia\",lit(\"1900-01-01\"))\n",
    "        df=df.withColumn(\"FechaFinVigencia\",lit(\"2300-01-01\"))\n",
    "        df=df.withColumn(\"VersionDelRegistro\",lit(\"S\"))\n",
    "        df=df.dropDuplicates()\n",
    "        df=df.sort(col(\"Sigla\"))\n",
    "        df = df.coalesce(1).withColumn(\"IDAeropuerto\", mid())\n",
    "        df.createOrReplaceTempView(\"df\")\n",
    "        df = spark.sql(\"SELECT INT(IDAeropuerto), STRING(Sigla), STRING(IATA), STRING(NombreAeropuerto),\\\n",
    "                        STRING(Ubicacion),string(Pais),\\\n",
    "                        STRING(Categoria),DOUBLE(Latitud), DOUBLE(Longitud),STRING(Municipio), STRING(df.Departamento), \\\n",
    "                         STRING(Propietario),STRING(Explotador),INT(LongitudPista), INT(AnchoPista),\\\n",
    "                        STRING(PBMO),INT(Elevacion), STRING(Resolucion), STRING(Clase),\\\n",
    "                        STRING(Tipo),STRING(GCD_Municipio), STRING(GCD_Departamento), DATE(df.FechaInicioVigencia),\\\n",
    "                       DATE(df.FechaFinVigencia), STRING(df.VersionDelRegistro),INT(df.Anio) from df\")\n",
    "        \n",
    "        x=df.count()\n",
    "        df=write_table(spark, server, database, 'DimAeropuerto', user,password,'overwrite',df)\n",
    "    else:\n",
    "        tablaDWH.persist()\n",
    "        df_fechas_antiguas=tablaDWH.selectExpr('Sigla','Sigla as SiglaAnt')\n",
    "        df_fechas_antiguas=df_fechas_antiguas.dropDuplicates()\n",
    "        df_nuevo=df.selectExpr('Sigla','Anio as AnioNuevo')\n",
    "        df_nuevo=df_nuevo.dropDuplicates()\n",
    "        df_temp=tablaDWH.join(df_nuevo, how = 'left', on = 'Sigla')\n",
    "        #registros que no tienen actualizacion\n",
    "        df_temp.persist()\n",
    "        df_mantener=df_temp.filter(df_temp.AnioNuevo.isNull())\n",
    "        df_mantener=df_mantener.drop('AnioNuevo')\n",
    "        df_mantener=df_mantener.withColumn(\"Origen\",lit(\"Mantener\"))\n",
    "        #registros que si tienen actualización\n",
    "        df_actualizar=df_temp.filter(df_temp.AnioNuevo.isNotNull())\n",
    "        #registros viejos que no van a cambiar\n",
    "        df_actualizar.persist()\n",
    "        df_actualizar_registrosviejos=df_actualizar.filter(df_actualizar.VersionDelRegistro==\"N\")\n",
    "        df_actualizar_registrosviejos=df_actualizar_registrosviejos.drop('AnioNuevo')\n",
    "        #actualización de registros que eran vigentes\n",
    "        df_actualizar_registrosvigentes=df_actualizar.filter(df_actualizar.VersionDelRegistro==\"S\")\n",
    "        df_actualizar_registrosvigentes=df_actualizar_registrosvigentes.withColumn(\"VersionDelRegistro\",lit(\"N\"))\n",
    "        df_actualizar_registrosvigentes=df_actualizar_registrosvigentes.withColumn(\"FechaFinVigencia\",\n",
    "                                                                                    sf.concat(sf.col('AnioNuevo')-1,sf.lit('-12-31'))\n",
    "                                                                                   )    \n",
    "        df_actualizar_registrosvigentes=df_actualizar_registrosvigentes.drop('AnioNuevo')\n",
    "        #registros nuevos para ingresar a la base\n",
    "        #Encontar llave máxima\n",
    "        max_key = tablaDWH.agg({\"IDAeropuerto\": \"max\"}).collect()[0][0]\n",
    "        df_nuevos_registros=df.alias('df_nuevos_registros')\n",
    "        df_nuevos_registros=df_nuevos_registros.join(df_fechas_antiguas,how = 'left', on = 'Sigla')\n",
    "        df_nuevos_registros=df_nuevos_registros.withColumn(\"FechaInicioVigencia\",when(df_nuevos_registros.SiglaAnt.isNull(),\\\n",
    "                                                                                     '1900-01-01')\\\n",
    "                                                           .otherwise(sf.concat(sf.col('Anio'),sf.lit('-01-01'))))\n",
    "        df_nuevos_registros=df_nuevos_registros.withColumn(\"FechaFinVigencia\",lit(\"2300-01-01\"))\n",
    "        df_nuevos_registros=df_nuevos_registros.withColumn(\"VersionDelRegistro\",lit(\"S\"))\n",
    "        df_nuevos_registros = df_nuevos_registros.withColumn('IDAeropuerto',  mid() + max_key+1)\n",
    "        #unir en un solo dataframe\n",
    "        df_mantener.createOrReplaceTempView(\"df_mantener\")\n",
    "        df_mantener = spark.sql(\"SELECT INT(IDAeropuerto), STRING(Sigla), STRING(IATA), STRING(NombreAeropuerto),\\\n",
    "                        STRING(Categoria),DOUBLE(Latitud), DOUBLE(Longitud),STRING(Municipio), STRING(df_mantener.Departamento), \\\n",
    "                        STRING(Propietario),STRING(Explotador),INT(LongitudPista), INT(AnchoPista),\\\n",
    "                        STRING(PBMO),INT(Elevacion), STRING(Resolucion), STRING(Clase),\\\n",
    "                        STRING(Ubicacion),string(Pais),\\\n",
    "                        STRING(Tipo),STRING(GCD_Municipio), STRING(GCD_Departamento), DATE(df_mantener.FechaInicioVigencia),\\\n",
    "                        DATE(df_mantener.FechaFinVigencia), STRING(df_mantener.VersionDelRegistro),INT(df_mantener.Anio)\\\n",
    "                        from df_mantener\")\n",
    "        \n",
    "        df_actualizar_registrosviejos.createOrReplaceTempView(\"df_actualizar_registrosviejos\")\n",
    "        df_actualizar_registrosviejos = spark.sql(\"SELECT INT(IDAeropuerto), STRING(Sigla), STRING(IATA), STRING(NombreAeropuerto),\\\n",
    "                        STRING(Categoria),DOUBLE(Latitud), DOUBLE(Longitud),STRING(Municipio), STRING(df_actualizar_registrosviejos.Departamento), \\\n",
    "                        STRING(Propietario),STRING(Explotador),INT(LongitudPista), INT(AnchoPista),\\\n",
    "                        STRING(PBMO),INT(Elevacion), STRING(Resolucion), STRING(Clase),\\\n",
    "                        STRING(Ubicacion),string(Pais),\\\n",
    "                        STRING(Tipo),STRING(GCD_Municipio), STRING(GCD_Departamento), DATE(df_actualizar_registrosviejos.FechaInicioVigencia),\\\n",
    "                        DATE(df_actualizar_registrosviejos.FechaFinVigencia),\\\n",
    "                        STRING(df_actualizar_registrosviejos.VersionDelRegistro),INT(df_actualizar_registrosviejos.Anio)\\\n",
    "                        from df_actualizar_registrosviejos\")\n",
    "        \n",
    "        df_actualizar_registrosvigentes.createOrReplaceTempView(\"df_actualizar_registrosvigentes\")\n",
    "        df_actualizar_registrosvigentes = spark.sql(\"SELECT INT(IDAeropuerto), STRING(Sigla), STRING(IATA), STRING(NombreAeropuerto),\\\n",
    "                        STRING(Categoria),DOUBLE(Latitud), DOUBLE(Longitud),STRING(Municipio), STRING(df_actualizar_registrosvigentes.Departamento), \\\n",
    "                         STRING(Propietario),STRING(Explotador),INT(LongitudPista), INT(AnchoPista),\\\n",
    "                        STRING(PBMO),INT(Elevacion), STRING(Resolucion), STRING(Clase),\\\n",
    "                        STRING(Ubicacion),string(Pais),\\\n",
    "                        STRING(Tipo),STRING(GCD_Municipio), STRING(GCD_Departamento), DATE(df_actualizar_registrosvigentes.FechaInicioVigencia),\\\n",
    "                       DATE(df_actualizar_registrosvigentes.FechaFinVigencia), STRING(df_actualizar_registrosvigentes.VersionDelRegistro),\\\n",
    "                       INT(df_actualizar_registrosvigentes.Anio) from df_actualizar_registrosvigentes\")\n",
    "        \n",
    "        df_nuevos_registros.createOrReplaceTempView(\"df_nuevos_registros\")\n",
    "        df_nuevos_registros = spark.sql(\"SELECT INT(IDAeropuerto), STRING(Sigla), STRING(IATA), STRING(NombreAeropuerto),\\\n",
    "                        STRING(Categoria),DOUBLE(Latitud), DOUBLE(Longitud),STRING(Municipio), STRING(df_nuevos_registros.Departamento), \\\n",
    "                         STRING(Propietario),STRING(Explotador),INT(LongitudPista), INT(AnchoPista),\\\n",
    "                        STRING(PBMO),INT(Elevacion), STRING(Resolucion), STRING(Clase),\\\n",
    "                        STRING(Ubicacion),string(Pais),\\\n",
    "                        STRING(Tipo),STRING(GCD_Municipio), STRING(GCD_Departamento), DATE(df_nuevos_registros.FechaInicioVigencia),\\\n",
    "                       DATE(df_nuevos_registros.FechaFinVigencia), STRING(df_nuevos_registros.VersionDelRegistro),\\\n",
    "                       INT(df_nuevos_registros.Anio) from df_nuevos_registros\")\n",
    "        \n",
    "        df2 = df_nuevos_registros.union(df_mantener)\n",
    "        df2 = df2.union(df_actualizar_registrosviejos)\n",
    "        df2 = df2.union(df_actualizar_registrosvigentes)\n",
    "        \n",
    "        x=df2.count()\n",
    "        df2=write_table(spark, server, database, 'DimAeropuerto', user,password,'overwrite',df2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61d4f851",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=actualizar_historia_aeropuertos(dfaeropuertosconhistoriaparte1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1bd9baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=actualizar_historia_aeropuertos(dfaeropuertosconhistoriaparte2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef8d3e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=actualizar_historia_aeropuertos(dfaeropuertosconhistoriaparte3)\n",
    "x=actualizar_historia_aeropuertos(dfaeropuertosconhistoriaparte4)\n",
    "x=actualizar_historia_aeropuertos(dfaeropuertosconhistoriaparte5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f36c74f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------------+---------+--------+------------------+-------------------+------+---------+----------+-----------+----------+---+\n",
      "|IDFecha|IDTipoVuelo|IDTipoTrafico|IDEmpresa|IDEquipo|IDAeropuertoOrigen|IDAeropuertoDestino|Vuelos|Pasajeros|CargaBordo|TotalSillas|TotalCarga| ID|\n",
      "+-------+-----------+-------------+---------+--------+------------------+-------------------+------+---------+----------+-----------+----------+---+\n",
      "|     76|          3|            1|       93|      62|              null|               null|     1|        0|         0|          0|         0|  0|\n",
      "|     17|          3|            1|       19|      70|              null|               null|     1|        2|       200|          0|         0|  1|\n",
      "|     38|          3|            1|      122|      61|              null|               null|     1|        2|       100|          0|         0|  2|\n",
      "|     13|          3|            1|      121|      38|              null|               null|    88|     1223|     14239|          0|         0|  3|\n",
      "|      7|          3|            1|      121|      38|              null|               null|    76|      295|     10639|          0|         0|  4|\n",
      "+-------+-----------+-------------+---------+--------+------------------+-------------------+------+---------+----------+-----------+----------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Creacion Tabla de Hechos Vuelos\n",
    "import pyspark.sql.functions as F\n",
    "DimAeropuerto = load_table(spark, server, database, 'DimAeropuerto', user, password)\n",
    "df_aeropuertoorigen=DimAeropuerto.selectExpr('Sigla as origen','Anio','FechaInicioVigencia',\n",
    "                                                     'FechaFinVigencia','IDAeropuerto as IDAeropuertoOrigen')\n",
    "df_aeropuertodestino=DimAeropuerto.selectExpr('Sigla as destino','Anio','FechaInicioVigencia',\n",
    "                                                      'FechaFinVigencia','IDAeropuerto as IDAeropuertoDestino')\n",
    "\n",
    "df_hechos_vuelos=dfvuelos.alias('df_hechos_vuelos')\n",
    "columns = ['vuelos', 'sillas','pasajeros','carga_bordo','carga_ofrecida']\n",
    "for column in columns:\n",
    "    df_hechos_vuelos = df_hechos_vuelos.withColumn(column,F.when(F.isnan(F.col(column)),0).otherwise(F.col(column)))\n",
    "df_hechos_vuelos= df_hechos_vuelos.withColumn(\"relativedate\",sf.concat(sf.col('ano'),lit(\"-\"),sf.col('mes'),sf.lit('-01')))\n",
    "df_hechos_vuelos= df_hechos_vuelos.withColumn(\"vuelos\",df_hechos_vuelos.vuelos.cast('int'))\n",
    "df_hechos_vuelos= df_hechos_vuelos.withColumn(\"sillas\",df_hechos_vuelos.sillas.cast('int'))\n",
    "df_hechos_vuelos= df_hechos_vuelos.withColumn(\"carga_ofrecida\",df_hechos_vuelos.carga_ofrecida.cast('int'))\n",
    "df_hechos_vuelos= df_hechos_vuelos.withColumn(\"carga_bordo\",df_hechos_vuelos.carga_bordo.cast('int'))\n",
    "df_hechos_vuelos= df_hechos_vuelos.withColumn(\"pasajeros\",df_hechos_vuelos.pasajeros.cast('int'))\n",
    "df_hechos_vuelos=df_hechos_vuelos.groupBy(\"relativedate\",\"ano\",\"mes\",\"origen\",\"destino\",\"tipo_equipo\",\"tipo_vuelo\",\"trafico\",\"empresa\") \\\n",
    "    .sum(\"vuelos\",\"pasajeros\",\"carga_bordo\",\"sillas\",\"carga_ofrecida\")\n",
    "df_hechos_vuelos=df_hechos_vuelos.withColumnRenamed(\"sum(vuelos)\", \"Vuelos\")\n",
    "df_hechos_vuelos=df_hechos_vuelos.withColumnRenamed(\"sum(pasajeros)\", \"Pasajeros\")\n",
    "df_hechos_vuelos=df_hechos_vuelos.withColumnRenamed(\"sum(carga_bordo)\", \"CargaBordo\")\n",
    "df_hechos_vuelos=df_hechos_vuelos.withColumnRenamed(\"sum(sillas)\", \"TotalSillas\")\n",
    "df_hechos_vuelos=df_hechos_vuelos.withColumnRenamed(\"sum(carga_ofrecida)\", \"TotalCarga\")\n",
    "df_hechos_vuelos.createOrReplaceTempView(\"df_hechos_vuelos\")\n",
    "df_aeropuertoorigen.createOrReplaceTempView(\"df_aeropuertoorigen\")\n",
    "df_aeropuertodestino.createOrReplaceTempView(\"df_aeropuertodestino\")\n",
    "df_fechames.createOrReplaceTempView(\"df_fechames\")\n",
    "df_tipo_vuelo.createOrReplaceTempView(\"df_tipo_vuelo\")\n",
    "df_empresatrans1.createOrReplaceTempView(\"df_empresatrans1\")\n",
    "df_trafico.createOrReplaceTempView(\"df_trafico\")\n",
    "df_tipoequipo.createOrReplaceTempView(\"df_tipoequipo\")\n",
    "df_hechos_vuelos = spark.sql(\"select  IDFecha,IDTipoVuelo,IDTipoTrafico,IDEmpresa,IDEquipo,IDAeropuertoOrigen,\\\n",
    "                             IDAeropuertoDestino,Vuelos,Pasajeros,CargaBordo,TotalSillas,TotalCarga from df_hechos_vuelos\\\n",
    "                             left join df_fechames on df_fechames.Anio= df_hechos_vuelos.ano and\\\n",
    "                             df_fechames.Mes= df_hechos_vuelos.mes\\\n",
    "                             left join df_tipo_vuelo on df_tipo_vuelo.CodigoVuelo= df_hechos_vuelos.tipo_vuelo\\\n",
    "                             left join df_trafico on df_trafico.Codigo_Trafico= df_hechos_vuelos.trafico\\\n",
    "                             left join df_empresatrans1 on df_empresatrans1.NombreEmpresa= df_hechos_vuelos.empresa\\\n",
    "                             left join df_tipoequipo on df_tipoequipo.NombreEquipo= df_hechos_vuelos.tipo_equipo\\\n",
    "                             left join df_aeropuertoorigen on df_aeropuertoorigen.origen= df_hechos_vuelos.origen and\\\n",
    "                             (df_hechos_vuelos.relativedate BETWEEN df_aeropuertoorigen.FechaInicioVigencia\\\n",
    "                             AND df_aeropuertoorigen.FechaFinVigencia)\\\n",
    "                             left join df_aeropuertodestino on df_aeropuertodestino.destino= df_hechos_vuelos.destino and\\\n",
    "                             (df_hechos_vuelos.relativedate BETWEEN df_aeropuertodestino.FechaInicioVigencia\\\n",
    "                             AND df_aeropuertodestino.FechaFinVigencia)\")\n",
    "df_hechos_vuelos = df_hechos_vuelos.coalesce(1).withColumn(\"ID\", mid())\n",
    "df_hechos_vuelos=write_table(spark, server, database, 'FactVuelos', user,password,'overwrite',df_hechos_vuelos)\n",
    "df_hechos_vuelos.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea382341",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
